---
title: "hw2-tora"
author: "Deepa, Tora"
date: "2022-09-26"
output:
  pdf_document: default
  html_document: default
---


```{r}
library(tidyr);
library(dplyr); 
library(kableExtra);

library(ggplot2)
```


## 1. Loaded classification data set from GitHub.

```{r}
data<- read.csv("https://raw.githubusercontent.com/deepasharma06/Data621-HW2/main/classification-output-data.csv")
head(data) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 15)
```

# 2. The data set has three key columns we will use:
 class: the actual class for the observation
 scored.class: the predicted class for the observation (based on a threshold of 0.5)
 scored.probability: the predicted probability of success for the observation

Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand 
the output. In particular, do the rows represent the actual or predicted class? The columns?


```{r confusion matrix}
#row: predicted value; columns: actual value
conf_matrix = table(Prediction = data$scored.class, Actual = data$class)
conf_matrix %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 15)
```



## 3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, 
     and returns the accuracy of the predictions.


```{r accuracy}
accuracy = function(data, predicted_col_name, actual_col_name) {
  
  conf = table(data[ , predicted_col_name], data[ , actual_col_name])
  TP = conf[2,2]
  TN = conf[1,1]
  FP = conf[2,1]
  FN = conf[1,2]
  
  #Accurary = (TP + TN) / (TP + FP +TN +FN)
  return(round((TP+TN)/(TP + FP + TN + FN), 4))
}
print(paste0("Accuracy: ", accuracy(data, 'scored.class', 'class')))
```

## 4.Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, 
and returns the classification error rate of the predictions.


Verify that you get an accuracy and an error rate that sums to one

```{r errorRate}
errorRate = function(data, predicted_col_name, actual_col_name) {
  
  conf = table(data[ , predicted_col_name], data[ , actual_col_name])
  TP = conf[2,2]
  TN = conf[1,1]
  FP = conf[2,1]
  FN = conf[1,2]
  
  #Classification Error Rate = ( FP + FN )/(TP + FP +TN +FN)
  return(round((FP+FN)/(TP + FP + TN + FN), 4))
}
print(paste0("Error rate: ", errorRate(data, 'scored.class', 'class')))
#accuracy + error rate
print(paste0("Accuracy + Error rate = ", accuracy(data, 'scored.class', 'class'), " + ", errorRate(data, 'scored.class', 'class'), " = ", (accuracy(data, 'scored.class', 'class') + errorRate(data, 'scored.class', 'class'))))
```





#5 Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.


```{r precision}
precision = function(data, predicted_col_name, actual_col_name) {
  
  conf = table(data[ , predicted_col_name], data[ , actual_col_name])
  TP = conf[2,2]
  TN = conf[1,1]
  FP = conf[2,1]
  FN = conf[1,2]
  
  #Precision = TP / (TP + FP)
  return(round((TP)/(TP + FP), 4))
}
print(paste0("Precision: ", precision(data, 'scored.class', 'class')))
```


## 6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

Sensitivity = TP / (TP + FN)




```{r sensitivity}
sensitivity = function(data, predicted_col_name, actual_col_name) {
  
  conf = table(data[ , predicted_col_name], data[ , actual_col_name])
  TP = conf[2,2]
  TN = conf[1,1]
  FP = conf[2,1]
  FN = conf[1,2]
  
  #Sensitivity = TP / (TP + FN)
  return(round((TP)/(TP + FN), 4))
}
print(paste0("Sensitivity: ", sensitivity(data, 'scored.class', 'class')))
```


## 7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

Specificity = TN / (TN+FP)


```{r specificity}
specificity = function(data, predicted_col_name, actual_col_name) {
  
  conf = table(data[ , predicted_col_name], data[ , actual_col_name])
  TP = conf[2,2]
  TN = conf[1,1]
  FP = conf[2,1]
  FN = conf[1,2]
  
  #Specificity = TN / (TN+FP)
  return(round((TN)/(TN + FP), 4))
}
print(paste0("Specificity: ", specificity(data, 'scored.class', 'class')))
```


# Tora
## 8.  Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the F1 score of the predictions.
𝐹1 𝑆𝑐𝑜𝑟𝑒 = 2 × 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦

```{r}
f1_score = function(data, predicted_col_name, actual_col_name) {
  p = precision(data, 'scored.class', 'class') 
  s = sensitivity(data, 'scored.class', 'class')
  return(2*p*s/(p+s))
} 
print(paste0("F1 Score: ", f1_score(data, 'scored.class', 'class')))
```




## 9. Before we move on, let’s consider a question that was asked: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. (Hint: If 0 < 𝑎 < 1 and 0 < 𝑏 < 1 then 𝑎𝑏 < 𝑎.)

The F1 score is equivalent to the following:

$$
p = precision, s = sensitivity
$$


$$
F1 = \frac{2*p*s}{p + s}
$$


Just by eye-balling this equation (and knowing that both precision and sensitivity are between 0 and 1), if we assume mutual exclusivity of both metrics, F1 would simplify to the following:

$$
p = 1, s = 1
\\
F1 = \frac{2*1*1}{1+1} = 1
$$

To figure out the minimum values, we can take the derivative of F1 and set it to zero. Since we have two metrics, we'll have to use partial derivatives:

$$
dF1/dp = 2s^2/(p+s)^2
$$

$$
dF1/ds = 2p^2/(p+s)^2
$$

If we set both values at 0, then precision and sensitivity would be at 0 for the zeroes of the function. If we go back and plug those values in, F1 would be 0. 

## 10.

Write a function that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example). Your function should return a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.


```{r ROC}

getROCcurve = function(col.true = "class", col.probability = "scored.probability", data = data) {
  vec.TPR = c()
  vec.TNR = c()

  for (i in seq(0, 1, 0.01)) {
    data = data %>% mutate(model.classification = ifelse(select(data, col.probability) < i, 0, 1))
    i.vec.TP = data %>% filter(model.classification == class & class == 1) %>% nrow()
    i.vec.FN = data %>% filter(model.classification != class & class == 1) %>% nrow()
    i.vec.TN = data %>% filter(model.classification == class & class == 0) %>% nrow()
    i.vec.FP = data %>% filter(model.classification != class & class == 0) %>% nrow()
    
    vec.TPR = c(vec.TPR, (i.vec.TP/(i.vec.TP + i.vec.FN)))
    vec.TNR = c(vec.TNR, (i.vec.TN/(i.vec.TN + i.vec.FP)))

  }
  df.ROC = data.frame(Threshold = seq(0, 1, 0.01), TNR = vec.TNR, TPR = vec.TPR)
  df.ROC = df.ROC %>% arrange(TNR, decreasing = T)
  plt.ROC = ggplot(aes(x = 1-(TNR), y = (TPR)), data = df.ROC) + geom_step() + labs(x = "1 - Specificity", y = "Sensitivity")
  df.AUC  = df.ROC %>% distinct(TPR, TNR)
  
  df.AUC <- df.AUC %>%  mutate(TNR_next = lead(TNR, n = 1L))  
  df.AUC$width = df.AUC$TNR_next - df.AUC$TNR
  vec.AUC = sum(df.AUC$width * df.AUC$TPR, na.rm = T)
  return(list("AUC" = vec.AUC, "ROC" = plt.ROC))
}


```











